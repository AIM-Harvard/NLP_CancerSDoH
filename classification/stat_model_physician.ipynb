{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import sklearn\n",
    "from utils import grab_sections\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords as stop_words\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = list(stop_words.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(notes: list[str]) -> list[str]:\n",
    "    clean_notes = []\n",
    "    punct_set = set(string.punctuation)\n",
    "    stop_word_set = set(stopwords)\n",
    "    for doc in notes:\n",
    "        doc = ' '.join(doc.split())\n",
    "        spac_doc = nlp(doc)\n",
    "        tokens = [tok.lemma_ for tok in spac_doc if tok.lemma_ not in punct_set and tok.lemma_ not in stop_word_set]\n",
    "        clean_notes.append(' '.join(tokens))\n",
    "    return clean_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(train_docs, train_labels):\n",
    "        label_counter = Counter(train_labels)\n",
    "        print('Original Train Label Counts: ')\n",
    "        print(label_counter)\n",
    "        majority_class = label_counter.most_common()[0][0]\n",
    "        minority_class = label_counter.most_common()[-1][0]\n",
    "        majority_count = label_counter.most_common()[0][1]\n",
    "        minority_count = label_counter.most_common()[-1][1]\n",
    "        majority_docs = [doc for i, doc in enumerate(train_docs) if train_labels[i]==majority_class]\n",
    "        minority_docs = [doc for i, doc in enumerate(train_docs) if train_labels[i]==minority_class]\n",
    "        majority_labels = [doc for doc in train_labels if doc==majority_class]\n",
    "        minority_labels = [doc for doc in train_labels if doc==minority_class]\n",
    "        assert(len(majority_docs)==len(majority_labels)==majority_count)\n",
    "        assert(len(minority_docs)==len(minority_labels)==minority_count)\n",
    "        majority_temp = list(zip(majority_docs, majority_labels))\n",
    "        random.shuffle(majority_temp)\n",
    "        maj_doc, maj_lab = zip(*majority_temp)\n",
    "        maj_doc = list(maj_doc)\n",
    "        maj_lab = list(maj_lab)\n",
    "        maj_doc = maj_doc[:minority_count]\n",
    "        maj_lab = maj_lab[:minority_count]\n",
    "        train_labels ,train_docs = [], []\n",
    "        train_labels.extend(minority_labels)\n",
    "        train_labels.extend(maj_lab)\n",
    "        train_docs.extend(minority_docs)\n",
    "        train_docs.extend(maj_doc)\n",
    "        new_counter = Counter(train_labels)\n",
    "        print('New Train Label Counts: ')\n",
    "        print(new_counter)\n",
    "        return train_docs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insurance Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2int = {'low':0, 'reg':1}\n",
    "int2lab = {0:'low', 1:'reg'}\n",
    "label = 'insurance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './insurance_data/insurance_train_phys.csv'\n",
    "dev_file = './insurance_data/insurance_dev_phys.csv'\n",
    "test_file = './insurance_data/insurance_test_phys.csv'\n",
    "train_df = pd.read_csv(train_file, encoding='utf8')\n",
    "dev_df = pd.read_csv(dev_file, encoding='utf8')\n",
    "test_df = pd.read_csv(test_file, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess: sectionize, lowercase, tokenize, lemmatize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes = [grab_sections(note, token_len=300).lower() for note in train_df['text'].to_list()]\n",
    "train_notes = clean_docs(train_notes)\n",
    "train_labels = [lab2int[lab] for lab in train_df[label].to_list()]\n",
    "dev_notes = [grab_sections(note, token_len=300).lower() for note in dev_df['text'].to_list()]\n",
    "dev_notes = clean_docs(dev_notes)\n",
    "dev_labels = [lab2int[lab] for lab in dev_df[label].to_list()]\n",
    "test_notes = [grab_sections(note, token_len=300).lower() for note in test_df['text'].to_list()]\n",
    "test_notes = clean_docs(test_notes)\n",
    "test_labels = [lab2int[lab] for lab in test_df[label].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes, train_labels = undersample(train_notes, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "train_x = bow_converter.fit_transform(train_notes)\n",
    "train_x = tf_transformer.fit_transform(train_x)\n",
    "dev_x = bow_converter.transform(dev_notes)\n",
    "dev_x = tf_transformer.transform(dev_x)\n",
    "test_x = bow_converter.transform(test_notes)\n",
    "test_x = tf_transformer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=seed_val, learning_rate='optimal')\n",
    "clf_sgd.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_sgd.predict(dev_x)\n",
    "print(classification_report(dev_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=1000, random_state=seed_val)\n",
    "clf_gb.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_preds = clf_gb.predict(dev_x)\n",
    "print(classification_report(dev_labels, gb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = clf_sgd.predict(test_x)\n",
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boosting\n",
    "gb_test_preds = clf_gb.predict(test_x)\n",
    "print(classification_report(test_labels, gb_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race / Ethnicity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2int = {'NonWhite':0, 'White_NonHispanic':1}\n",
    "int2lab = {0:'NonWhite', 1:'White_NonHispanic'}\n",
    "label = 'Race_group'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './race_eth_data/race_train_phys.csv'\n",
    "dev_file = './race_eth_data/race_dev_phys.csv'\n",
    "test_file = './race_eth_data/race_test_phys.csv'\n",
    "train_df = pd.read_csv(train_file, encoding='utf8')\n",
    "dev_df = pd.read_csv(dev_file, encoding='utf8')\n",
    "test_df = pd.read_csv(test_file, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess: sectionize, lowercase, tokenize, lemmatize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes = [grab_sections(note, token_len=300).lower() for note in train_df['text'].to_list()]\n",
    "train_notes = clean_docs(train_notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [1 if lab=='White_NonHispanic' else 0 for lab in train_df[label].to_list()]\n",
    "dev_notes = [grab_sections(note, token_len=300).lower() for note in dev_df['text'].to_list()]\n",
    "dev_notes = clean_docs(dev_notes)\n",
    "dev_labels = [1 if lab=='White_NonHispanic' else 0  for lab in dev_df[label].to_list()]\n",
    "test_notes = [grab_sections(note, token_len=300).lower() for note in test_df['text'].to_list()]\n",
    "test_notes = clean_docs(test_notes)\n",
    "test_labels = [1 if lab=='White_NonHispanic' else 0  for lab in test_df[label].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes, train_labels = undersample(train_notes, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "train_x = bow_converter.fit_transform(train_notes)\n",
    "train_x = tf_transformer.fit_transform(train_x)\n",
    "dev_x = bow_converter.transform(dev_notes)\n",
    "dev_x = tf_transformer.transform(dev_x)\n",
    "test_x = bow_converter.transform(test_notes)\n",
    "test_x = tf_transformer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=seed_val, learning_rate='optimal')\n",
    "clf_sgd.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_sgd.predict(dev_x)\n",
    "print(classification_report(dev_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=1000, random_state=seed_val)\n",
    "clf_gb.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_preds = clf_gb.predict(dev_x)\n",
    "print(classification_report(dev_labels, gb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = clf_sgd.predict(test_x)\n",
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boosting\n",
    "gb_test_preds = clf_gb.predict(test_x)\n",
    "print(classification_report(test_labels, gb_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2int = {'Female':0, 'Male':1}\n",
    "int2lab = {0:'Female', 1:'Male'}\n",
    "label = 'Gender'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './gender_data/gen_train_phys.csv'\n",
    "dev_file = './gender_data/gen_dev_phys.csv'\n",
    "test_file = './gender_data/gen_test_phys.csv'\n",
    "train_df = pd.read_csv(train_file, encoding='utf8')\n",
    "dev_df = pd.read_csv(dev_file, encoding='utf8')\n",
    "test_df = pd.read_csv(test_file, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess: sectionize, lowercase, tokenize, lemmatize, remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes = [grab_sections(note, token_len=300).lower() for note in train_df['text'].to_list()]\n",
    "train_notes = clean_docs(train_notes)\n",
    "train_labels = [lab2int[lab] for lab in train_df[label].to_list()]\n",
    "dev_notes = [grab_sections(note, token_len=300).lower() for note in dev_df['text'].to_list()]\n",
    "dev_notes = clean_docs(dev_notes)\n",
    "dev_labels = [lab2int[lab] for lab in dev_df[label].to_list()]\n",
    "test_notes = [grab_sections(note, token_len=300).lower() for note in test_df['text'].to_list()]\n",
    "test_notes = clean_docs(test_notes)\n",
    "test_labels = [lab2int[lab] for lab in test_df[label].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notes, train_labels = undersample(train_notes, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer()\n",
    "train_x = bow_converter.fit_transform(train_notes)\n",
    "train_x = tf_transformer.fit_transform(train_x)\n",
    "dev_x = bow_converter.transform(dev_notes)\n",
    "dev_x = tf_transformer.transform(dev_x)\n",
    "test_x = bow_converter.transform(test_notes)\n",
    "test_x = tf_transformer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(loss='log', penalty='l2', max_iter=1000, random_state=seed_val, learning_rate='optimal')\n",
    "clf_sgd.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_sgd.predict(dev_x)\n",
    "print(classification_report(dev_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = GradientBoostingClassifier(loss='deviance', learning_rate=0.01, n_estimators=1000, random_state=seed_val)\n",
    "clf_gb.fit(train_x, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_preds = clf_gb.predict(dev_x)\n",
    "print(classification_report(dev_labels, gb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = clf_sgd.predict(test_x)\n",
    "print(classification_report(test_labels, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient Boosting\n",
    "gb_test_preds = clf_gb.predict(test_x)\n",
    "print(classification_report(test_labels, gb_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bwh_models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d28981372adf6627f31be41cb660cbd4a9050764ba0e8d7de31ed03d8776b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
